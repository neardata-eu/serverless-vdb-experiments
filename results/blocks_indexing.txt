EXPERIMENT:
    Indexing with blocks implementation

RESOURCES:
    AWS S3: upload dataset and true neighbors .csv files
    AWS Lambda: create runtime with all needed packages (from Dockerfile.lambda)
    AWS EC2: create c7i.xlarge from where tests will be executed. Leave everything by default except for security groups, where port 22 ssh access inbound connections must be allowed

STEPS:
    1. Upload code to ec2 instance
    2. Install all required packages
           pip
             lithops
             faiss-cpu
             numpy
             pandas
             boto3
    3. Create .lithops_config file
    4. Create runtime from ec2 instance
    5. Run the following command:
           python3 example.py --features <dataset_dimension> --skip_query --impl blocks --dataset <dataset_name> --num_index <number_of_partitions> --k 512 --n_probe 32 --storage_bucket <s3_bucket_name> --indexing_memory 10240

NOTES:
    For indexing sift dataset, a line in code must be changed. vectordb/indexing.py L214:
        obj_chunk = 16 -> obj_chunk = 32
    Also, sift dataset indexing must start from 32 indexes
    Every time a new lambda function with specific memory is invoked it needs to be deployed, adding extra overhead. It is recommended to let the experiment finish and then discard the results. 
    When deploying a new function, it is recommended to increase the storage size from 512MB default to 2048MB
